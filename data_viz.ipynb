{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Geospatial Data Visualization with Python\n",
    "\n",
    "## ABSTRACT\n",
    "\n",
    "This chapter explores modern approaches to geospatial data visualization using Python libraries. We present a standardized framework for working with both vector and raster geospatial data through libraries like GeoPandas, Rasterio, Leafmap, and Geemap. The chapter progresses from fundamental concepts to practical implementation, providing researchers, data scientists, and GIS professionals with the tools needed to effectively visualize and analyze geospatial information. Through practical examples and best practices, readers will learn to create dynamic, interactive, and information-rich visualizations that support better understanding of spatial relationships and patterns in geospatial data.\n",
    "\n",
    "**KEYWORDS**\n",
    "\n",
    "Geospatial visualization, Python, GeoPandas, Rasterio, Leafmap, Geemap, interactive mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## INTRODUCTION\n",
    "\n",
    "Geospatial data visualization represents the intersection of geographic information science (GIS), data science, and visual communication. It transforms complex spatial relationships and patterns into accessible visual representations that facilitate understanding, analysis, and decision-making. The field has evolved dramatically with advances in computing power, data availability, and open-source software development, creating new opportunities for innovation across disciplines ranging from environmental science to urban planning, public health, and disaster management.\n",
    "\n",
    "The ability to effectively visualize geospatial data has become increasingly important as organizations collect and analyze larger volumes of location-based information. Whether tracking environmental changes, optimizing supply chains, analyzing demographic patterns, or responding to natural disasters, the capacity to represent spatial data visually provides critical insights that might otherwise remain hidden in tables of coordinates and attributes.\n",
    "\n",
    "Python has emerged as a leading programming language for geospatial data science due to its readability, extensive ecosystem of libraries, and strong community support. The rise of artificial intelligence and deep learning has further cemented Python's dominance in geospatial applications. The language's widespread adoption in the AI community, powered by frameworks like TensorFlow, PyTorch, and scikit-learn, has created natural synergies with geospatial analysis. These machine learning capabilities enable automated feature extraction from satellite imagery, predictive modeling of environmental phenomena, and sophisticated pattern recognition in spatial data—tasks that were previously labor-intensive or computationally prohibitive.\n",
    "\n",
    "The convergence of deep learning and geospatial technologies in Python has revolutionized how we process and interpret Earth observation data. Convolutional Neural Networks (CNNs) now routinely classify land cover from multispectral imagery, detect objects in aerial photographs, and segment complex landscapes with remarkable accuracy. Meanwhile, Recurrent Neural Networks (RNNs) and their variants help analyze temporal patterns in geospatial data sequences, from climate trends to urban development trajectories.\n",
    "\n",
    "\n",
    "Four Python libraries in particular have transformed how practitioners work with geospatial data:\n",
    "\n",
    "1. **GeoPandas** extends the popular pandas data analysis library to support geometric data types and spatial operations, making it ideal for working with vector data like points, lines, and polygons.\n",
    "\n",
    "2. **Rasterio** provides a high-level interface to GDAL (Geospatial Data Abstraction Library) for reading and writing raster datasets such as satellite imagery, digital elevation models, and other gridded data.\n",
    "\n",
    "3. **Leafmap** builds on established mapping libraries to offer a user-friendly interface for creating interactive web maps and visualizing both vector and raster data.\n",
    "\n",
    "4. **Geemap** enables easy access to and analysis of Google Earth Engine's vast repository of geospatial data, allowing Python users to harness the power of cloud computing for large-scale geospatial analysis and visualization. \n",
    "\n",
    "Together, these tools enable practitioners to process, analyze, and visualize geospatial data with unprecedented ease and flexibility. They support workflows ranging from exploratory data analysis to the creation of publication-quality static maps and interactive web applications.\n",
    "\n",
    "This chapter explores how these libraries can be used individually and in combination to create effective geospatial visualizations. We begin with the fundamental concepts and data structures that underpin geospatial data in Python, then progress to specific visualization techniques for both vector and raster data. Throughout, we emphasize practical implementation with clear code examples and best practices to guide readers in developing their own visualization workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## METHODOLOGY\n",
    "\n",
    "This section introduces the fundamental principles and practical application of geospatial data visualization using Python.  We'll guide you through the essential concepts, core Python libraries, and the underlying approaches to effectively represent spatial data. Designed for readers without a background in Geoscience or Remote Sensing, this guide emphasizes a clear, step-by-step explanation alongside practical code examples.  Our primary goal is to equip you with the knowledge and skills necessary to create insightful and informative geospatial visualizations with a few lines of code.\n",
    "\n",
    "### Setting Up the Python Environment\n",
    "\n",
    "Before embarking on geospatial visualization, it's crucial to create a virtual environment containing the necessary libraries. This section outlines two recommended approaches for creating such an environment: using the `conda` package manager and the newer, high-performance `uv` package manager.\n",
    "\n",
    "#### Using conda for Environment Setup\n",
    "\n",
    "[Conda](https://docs.conda.io/) is a popular package manager that handles library dependencies efficiently, especially for complex scientific packages with compiled components. The following steps will guide you through setting up a dedicated geospatial environment using conda:\n",
    "\n",
    "1. **Install Miniconda or Anaconda**: Begin by installing either [Miniconda](https://www.anaconda.com/docs/getting-started/miniconda/main) (a minimal version) or [Anaconda](https://www.anaconda.com/download) (a comprehensive distribution with many pre-installed packages). Miniconda is recommended for a leaner starting point.\n",
    "\n",
    "2. **Create a new conda environment**: Open your terminal (or Anaconda Prompt on Windows) and execute the following command to create a new environment specifically for your geospatial projects:\n",
    "\n",
    "```bash\n",
    "conda create -n geoenv python\n",
    "```\n",
    "This command creates a new conda environment named `geoenv` with the latest version of Python. You can specify a particular Python version if needed (e.g., `python=3.12`).\n",
    "\n",
    "3. **Activate the environment**: Once the environment is created, you need to activate it to start using it. Run the following command:\n",
    "\n",
    "```bash\n",
    "conda activate geoenv\n",
    "```\n",
    "This command activates the `geoenv` environment, allowing you to install and manage packages within this isolated space.\n",
    "\n",
    "4. **Install geospatial libraries**: Leverage the `conda-forge` channel, a community-driven repository known for providing up-to-date and well-maintained geospatial packages.  Use the following command to install essential libraries:\n",
    "\n",
    "```bash\n",
    "conda install -c conda-forge geopandas rasterio leafmap geemap maplibre\n",
    "```\n",
    "\n",
    "This command installs `geopandas` (for vector data), `rasterio` (for raster data), `leafmap` (for interactive maps), `geemap` (for Google Earth Engine integration), and `maplibre` (for 3D visualization). The `-c conda-forge` flag specifies that packages should be installed from the conda-forge channel, which typically has the most up-to-date versions of geospatial packages with proper dependency management.\n",
    "\n",
    "#### Using uv for Environment Setup\n",
    "\n",
    "[uv](https://github.com/astral-sh/uv) is a modern, extremely fast Python package installer and resolver. Its efficiency makes it an attractive alternative to `conda`, particularly for projects where speed is a concern. The following steps detail how to set up a geospatial environment using `uv`:\n",
    "\n",
    "1. **Install uv**: If you haven't already, install uv using the provided installation scripts for your operating system:\n",
    "\n",
    "```bash\n",
    "# For Windows\n",
    "powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n",
    "\n",
    "# For macOS and Linux\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "```\n",
    "\n",
    "2. **Create a virtual environment**: `uv` creates standard Python virtual environments. Create one in your project directory:\n",
    "\n",
    "```bash\n",
    "uv venv\n",
    "```\n",
    "\n",
    "This command creates a new virtual environment with the default Python version in the current directory. The virtual environment will be stored in a folder named `.venv` by default. You can specify a particular Python version if needed (e.g., `uv venv --python 3.12`).\n",
    "\n",
    "3. **Install geospatial libraries**:  Use `uv pip` to install the required geospatial libraries within the activated virtual environment:\n",
    "\n",
    "```bash\n",
    "uv pip install geopandas rasterio leafmap geemap maplibre\n",
    "```\n",
    "\n",
    "Unlike conda, which manages complex binary dependencies, `uv` primarily manages Python packages. However, its significantly faster installation speeds (often 10x faster than traditional pip) make it an excellent choice for projects where the underlying system dependencies are already available.\n",
    "\n",
    "#### Package Installation Notes\n",
    "\n",
    "Installing geospatial packages can sometimes present challenges due to their reliance on system-level dependencies.  Consider these points based on your operating system:\n",
    "\n",
    "-   **Windows**:  `conda` generally offers a more streamlined experience for Windows users, as it often provides pre-compiled binaries that handle system-level dependencies automatically.\n",
    "\n",
    "-   **Linux**:  You may need to install additional system libraries manually. On Debian-based systems like Ubuntu, the following command can often resolve dependency issues:\n",
    "\n",
    "    ```bash\n",
    "    sudo apt-get install libgdal-dev libproj-dev proj-bin libgeos-dev\n",
    "    ```\n",
    "\n",
    "-   **macOS**:  Using `conda` is frequently the easiest approach on macOS.  Alternatively, Homebrew can be used to install dependencies, but `conda` often provides a more self-contained solution.\n",
    "\n",
    "\n",
    "#### JupyterLab Setup\n",
    "\n",
    "For an improved interactive development experience, we recommend using JupyterLab over the classic Jupyter Notebook. JupyterLab provides a more modern and feature-rich environment for working with geospatial data and visualizations.\n",
    "\n",
    "```bash\n",
    "# Install Jupyter Lab\n",
    "conda install -c conda-forge jupyterlab  # or: uv pip install jupyterlab\n",
    "```\n",
    "\n",
    "Launch JupyterLab using the following command:\n",
    "\n",
    "```bash\n",
    "jupyter lab\n",
    "```\n",
    "\n",
    "Once JupyterLab is running, you can access it through your web browser. Create a new notebook to start working with geospatial data. JupyterLab's interface allows you to easily manage multiple notebooks, text editors, and terminal sessions within a single window, enhancing your productivity and workflow.\n",
    "\n",
    "\n",
    "#### Testing Your Installation\n",
    "\n",
    "To ensure that your Python environment is correctly set up with all the geospatial libraries, let's create a new code block in a Jupyter Notebook and run a simplele test script. This script will verify that the required libraries can be imported and that basic functionality is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test imports\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "import leafmap\n",
    "import geemap\n",
    "\n",
    "# Print versions\n",
    "print(f\"GeoPandas version: {gpd.__version__}\")\n",
    "print(f\"Rasterio version: {rasterio.__version__}\")\n",
    "print(f\"Leafmap version: {leafmap.__version__}\")\n",
    "print(f\"Geemap version: {geemap.__version__}\")\n",
    "\n",
    "# Create a simple map\n",
    "m = leafmap.Map()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "If the script executes without any errors and displays an interactive map, it confirms that your environment is properly configured and ready for geospatial data visualization. The import statements load the necessary libraries, the print statements display the version numbers, and the `leafmap.Map()` call creates a basic interactive map, which is the ultimate test of your setup. The interactive map should appear directly in your notebook, with zoom controls and a basemap showing a world map.\n",
    "\n",
    "### Fundamental Geospatial Data Types\n",
    "\n",
    "Geospatial data, the information tied to specific locations on Earth, comes in two primary flavors: vector and raster. Understanding these data types is crucial for choosing the right tools and techniques for visualization and analysis.\n",
    "\n",
    "#### Vector Data\n",
    "\n",
    "Vector data represents geographic features as discrete geometric objects. Think of it like drawing shapes on a map. There are three main types of vector data:\n",
    "\n",
    "*   **Points**: Represent single locations with a specific latitude and longitude. Examples include cities, landmarks, or the location of individual trees.\n",
    "*   **Lines**: Represent linear features like roads, rivers, or power lines. They are defined by a series of connected points.\n",
    "*   **Polygons**: Represent areas with boundaries, such as countries, lakes, or buildings. They are defined by a closed loop of connected points.\n",
    "\n",
    "Vector data is typically stored in file formats such as:\n",
    "\n",
    "*   **Shapefiles**: A common, older format. It consists of multiple files with different extensions (e.g., .shp, .shx, .dbf) that together represent the geometric and attribute data.\n",
    "*   **GeoJSON**: A lightweight, text-based format. It's easy to read and write, making it popular for web applications. GeoJSON represents geographic features as JSON objects, which can be easily manipulated in web-based applications.\n",
    "*   **GeoPackage**: A modern, single-file format that supports both vector and raster data. It is an open standard format that can store multiple layers of geospatial data in a single file, making it efficient for storage and sharing.\n",
    "*   **GeoParquet**: A column-oriented format for efficient storage and querying. It is designed for high-performance geospatial data storage and retrieval, especially for large datasets. GeoParquet is built on top of the Apache Parquet format, which is optimized for efficient data compression and encoding.\n",
    "\n",
    "These formats contain both the geometric information (the points, lines, or polygons) and associated attributes (e.g., the name of a city, the length of a road, the population of a country).\n",
    "\n",
    "#### Raster Data\n",
    "\n",
    "Raster data represents continuous surfaces as a grid of cells or pixels. Imagine a photograph of the Earth taken from space. Each pixel in the image has a value representing a specific characteristic of that location. Common examples include:\n",
    "\n",
    "*   **Satellite imagery**: Images captured by satellites, showing the Earth's surface in different wavelengths of light.\n",
    "*   **Elevation models**: Digital representations of terrain, where each pixel's value represents the elevation at that location.\n",
    "*   **Climate data**: Data representing temperature, rainfall, or other climate variables, where each pixel's value represents the value of that variable at that location.\n",
    "\n",
    "Raster data is typically stored in file formats such as:\n",
    "\n",
    "*   **GeoTIFF**: A widely used format for storing georeferenced raster data.\n",
    "*   **NetCDF**: A format commonly used for storing scientific data, including climate and oceanographic data.\n",
    "*   **Cloud-Optimized GeoTIFF (COG)**: A variant of GeoTIFF that is optimized for cloud storage and access, enabling efficient retrieval of specific parts of the image. \n",
    "\n",
    "### Working with Vector Data: GeoPandas\n",
    "\n",
    "[GeoPandas](https://geopandas.org) is a powerful Python library that extends the capabilities of the popular `pandas` library to handle geospatial vector data. Essentially, it adds a \"geometry\" column to `pandas` DataFrames, turning them into `GeoDataFrames` that support spatial operations.\n",
    "\n",
    "#### Loading Vector Data\n",
    "\n",
    "The first step in working with vector data is loading it into a GeoDataFrame. GeoPandas can read from various file formats including Shapefiles, GeoJSON, and GeoPackage. Let's start by importing the library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Next, we'll load a GeoJSON file containing data about New York City boroughs from a remote URL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_path = (\n",
    "    \"https://github.com/opengeos/datasets/releases/download/vector/nybb.geojson\"\n",
    ")\n",
    "gdf = gpd.read_file(vector_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "The `read_file()` function automatically detects the file format based on the file extension or URL path. It supports various geospatial formats beyond GeoJSON, including Shapefiles, GeoPackages, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Exploring and Manipulating Vector Data\n",
    "\n",
    "Once your data is loaded into a GeoDataFrame, you can explore its contents and structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "This displays the first five rows of the GeoDataFrame, showing both the attribute columns (`BoroCode`, `BoroName`) and the special `geometry` column that contains the spatial data. The output would show a table with borough information and a `geometry` column containing complex polygon objects.\n",
    "\n",
    "To understand what coordinate system the data uses, check the Coordinate Reference System (CRS):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(gdf.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "The CRS defines how the coordinates in the `geometry` column relate to locations on the Earth's surface. It's crucial to know the CRS when combining multiple datasets or performing spatial operations.\n",
    "\n",
    "GeoPandas provides many spatial operations. Let's create a buffer around each borough:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffered = gdf.buffer(distance=1000)  # Create 1km buffer around features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "This operation creates a new geometry collection where each borough polygon has been expanded outward by 1,000 meters (1 kilometer) in all directions. Buffering is a common operation in spatial analysis. The `buffer()` method creates expanded polygons by drawing a circle of the specified radius around each vertex of the original geometry and then creating a smooth outline that encompasses all these circles. The result is a new set of geometries that are larger than the originals but maintain a similar shape.\n",
    "\n",
    "#### Visualizing Vector Data\n",
    "\n",
    "GeoPandas makes it easy to create static maps from your vector data using the `plot()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plot\n",
    "gdf.plot(figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "This creates a simple map showing the borough boundaries. The `figsize` parameter sets the size of the resulting figure in inches (width, height). The output is a basic map with each borough shown as a filled polygon in the same color.\n",
    "\n",
    "For more informative visualizations, we can create thematic maps where features are colored according to their attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = gdf.plot(\n",
    "    column=\"BoroName\", cmap=\"viridis\", legend=True, figsize=(10, 6), edgecolor=\"black\"\n",
    ")\n",
    "ax.get_legend().set_loc(\"upper left\")  # Set legend location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "This code creates a choropleth map where:\n",
    "\n",
    "- Each borough is colored according to its name (`column='BoroName'`)\n",
    "- The color scheme uses the 'viridis' colormap (`cmap='viridis'`)\n",
    "- Borough boundaries are outlined in black (`edgecolor='black'`) \n",
    "- A legend is added showing which color corresponds to which borough (`legend=True`)\n",
    "- The legend is positioned in the upper left corner of the map\n",
    "\n",
    "The `plot()` method returns a matplotlib axes object (`ax`), which we can further customize. Here, we reposition the legend from the default upper-right position to the upper-left position to avoid covering important parts of the map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### Working with Raster Data: Rasterio\n",
    "\n",
    "Rasterio is a Python library that provides a clean and efficient way to read, manipulate, and write raster data. It's built on top of GDAL, a powerful geospatial data processing library, but offers a more Pythonic interface.\n",
    "\n",
    "#### Loading Raster Data\n",
    "\n",
    "To work with raster data, first import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import rasterio.plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "Now let's open a Digital Elevation Model (DEM) raster file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_path = (\n",
    "    \"https://github.com/opengeos/datasets/releases/download/raster/dem_90m.tif\"\n",
    ")\n",
    "src = rasterio.open(raster_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "This above code:\n",
    "\n",
    "1. Defines the URL path to a GeoTIFF file containing elevation data\n",
    "2. Uses `rasterio.open()` to load the raster dataset\n",
    "3. Assigns the opened dataset to the variable `src`\n",
    "\n",
    "Unlike GeoPandas, which loads the entire dataset into memory, Rasterio keeps a connection to the data source and only reads data as needed, making it more memory-efficient for large raster datasets.\n",
    "\n",
    "#### Accessing Raster Metadata\n",
    "\n",
    "Raster files contain important metadata about their content and structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "src.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "This displays a dictionary containing essential information about the raster, including:\n",
    "\n",
    "- `width` and `height`: The dimensions of the raster in pixels\n",
    "- `count`: The number of bands (channels) in the raster\n",
    "- `dtype`: The data type of the pixel values (e.g., float32, uint8)\n",
    "- `crs`: The coordinate reference system\n",
    "- `transform`: The affine transformation that maps pixel coordinates to geographic coordinates\n",
    "\n",
    "To get the spatial resolution (pixel size), use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "src.res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "This returns a tuple of (x_resolution, y_resolution), typically in the units of the CRS (often meters). For example, (90.0, 90.0) would indicate 90-meter pixels in both directions.\n",
    "\n",
    "#### Visualizing Raster Data\n",
    "\n",
    "Rasterio provides functions for visualizing raster data using Matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "rasterio.plot.show(src, cmap=\"terrain\", ax=ax, title=\"Digital Elevation Model (DEM)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "This abo e code:\n",
    "\n",
    "1. Creates a matplotlib figure and axes with a size of 8×8 inches\n",
    "2. Uses `rasterio.plot.show()` to display the raster data on the axes\n",
    "3. Sets the color palette to \"terrain\" which is suitable for elevation data (using shades of green for low elevations and brown/white for higher elevations)\n",
    "4. Adds a title to the plot\n",
    "5. Displays the plot\n",
    "\n",
    "The `rasterio.plot.show()` function automatically handles the transformation between the raster's internal coordinate system and the plot's coordinate system, ensuring the data is displayed correctly.\n",
    "\n",
    "#### Processing Raster Data\n",
    "\n",
    "Raster data can be processed and manipulated as NumPy arrays, enabling powerful calculations and transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_band = src.read(1)\n",
    "mountain = dem_band > 3000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "The first line reads the first band (indexed from 1) of the raster into a NumPy array called `dem_band`. For a single-band raster like a DEM, there's only one band containing elevation values.\n",
    "\n",
    "The second line creates a boolean mask called `mountain` where:\n",
    "- Each element is `True` where the corresponding elevation value is greater than 3000 meters\n",
    "- Each element is `False` where the elevation is 3000 meters or less\n",
    "\n",
    "This simple thresholding operation creates a binary mask identifying high-elevation areas (mountains). Similar techniques can be used to identify other features in raster data, such as water bodies, forested areas, or urban development.\n",
    "\n",
    "#### Visualizing Processed Raster Data\n",
    "\n",
    "We can visualize the results of our processing:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "rasterio.plot.show(\n",
    "    mountain, ax=ax, title=\"Mountainous Region (Elevation > 3,000 m)\", cmap=\"gray\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "This displays our binary `mountain` mask as a black and white image, where:\n",
    "- White pixels represent mountainous areas (elevation > 3000m)\n",
    "- Black pixels represent non-mountainous areas\n",
    "\n",
    "This simple visualization effectively highlights regions of high elevation, making it easy to identify mountain ranges. The `cmap=\"gray\"` parameter specifies a grayscale colormap, appropriate for binary data. You can also use any other colormap supported by Matplotlib, such as \"viridis\" or \"plasma\", to create more visually appealing representations.\n",
    "\n",
    "### Interactive Mapping: Leafmap\n",
    "\n",
    "GeoPandas and Rasterio are excellent for static maps and data manipulation, but for interactive web maps, we turn to Leafmap. Leafmap is a Python library designed for creating interactive maps with minimal code. It allows you to visualize both vector and raster data in a Jupyter environment. Leafmap is built on top of popular libraries like `ipyleaflet` and `folium`. It provides a high-level API for creating maps, adding layers, and interacting with map elements.\n",
    "\n",
    "#### Creating a Base Map\n",
    "\n",
    "Let's start by importing the library and creating a simple interactive map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leafmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Now we can create a basic interactive map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = leafmap.Map(center=[20, 0], zoom=2)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "This abvoe code:\n",
    "\n",
    "1. Creates a new Leafmap map object\n",
    "2. Centers it at latitude 20°, longitude 0° (near the equator in Africa)\n",
    "3. Sets the initial zoom level to 2 (showing most of the world)\n",
    "4. Displays the map in the notebook\n",
    "\n",
    "The resulting map is fully interactive - you can zoom in/out, pan around, and view the coordinates of any location. By default, Leafmap uses the OpenStreetMap basemap, which provides a simple view of the world with basic features like country boundaries, major cities, and transportation networks.\n",
    "\n",
    "#### Adding Basemaps\n",
    "\n",
    "Leafmap provides access to numerous basemaps from providers like Esri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_basemap(\"Esri.WorldImagery\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "This adds the Esri World Imagery basemap, which provides high-resolution satellite imagery for the entire world. The map now displays detailed aerial/satellite imagery instead of the default OpenStreetMap style. Users can select different basemaps using the toolbar button that appears in the top-right corner of the map. Note that new layers will be added to map displayed previously, so you may need to scroll up to the see newly added basemap. \n",
    "\n",
    "#### Adding Vector Data\n",
    "\n",
    "You can add vector data to your Leafmap map using the `add_vector()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_path = (\n",
    "    \"https://github.com/opengeos/datasets/releases/download/us/us_state_20m.geojson\"\n",
    ")\n",
    "m.add_vector(vector_path, layer_name=\"US States\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "The `add_vector()` function performs several operations behind the scenes:\n",
    "\n",
    "- Downloads the GeoJSON data from the specified URL\n",
    "- Converts it to a format that can be displayed on web maps\n",
    "- Adds it as an interactive layer to the map\n",
    "- Automatically styles the features (states in this case) with a default fill color and outline\n",
    "\n",
    "The resulting map shows US state boundaries overlaid on the basemap. You can hover your mouse on any state to view its attributes (properties from the GeoJSON file) in a popup in the lower-right corner. You can also toggle the visibility of this layer using the layer control panel.\n",
    "\n",
    "#### Adding Raster Data\n",
    "\n",
    "Leafmap can also display raster data as an overlay:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster_path = \"https://github.com/opengeos/datasets/releases/download/raster/LC09_039035_20240708_90m.tif\"\n",
    "m.add_raster(raster_path, indexes=[5, 4, 3], layer_name=\"Landsat 9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "The `add_raster()` function:\n",
    "\n",
    "1. Retrieves the raster data from the specified URL\n",
    "2. Applies the band combination specified by `indexes`\n",
    "3. Contructs a tile layer from the raster data\n",
    "4. Adds it as an interactive layer to the map\n",
    "\n",
    "This particular band combination (5-4-3) makes vegetation appear bright red, urban areas appear cyan, and water appears dark blue or black. Different band combinations highlight different features on the Earth's surface.\n",
    "\n",
    "### Cloud-based Geospatial Analysis: Geemap\n",
    "\n",
    "Geemap bridges the gap between Python and the Google Earth Engine (GEE) platform, enabling users to access and process massive geospatial datasets in the cloud. GEE hosts petabytes of satellite imagery and other geospatial data, combined with powerful computational resources for processing this data at scale.\n",
    "\n",
    "#### Setting up Geemap\n",
    "\n",
    "Before using Geemap, you need to initialize it and authenticate with Google Earth Engine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Initialize the Earth Engine API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "geemap.ee_initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "This above line:\n",
    "\n",
    "1. Checks if you're already authenticated with Google Earth Engine\n",
    "2. If not, initiates the OAuth2 authentication process\n",
    "3. Establishes a connection to the Earth Engine servers\n",
    "\n",
    "The first time you run this, you'll be prompted to authenticate with your Google account that has Earth Engine access. After authentication, you can create a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = geemap.Map(center=[20, 0], zoom=2)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "This creates an interactive map similar to Leafmap, but with added capabilities for working with Earth Engine data. The map includes Earth Engine's layer control system in addition to the standard mapping interface.\n",
    "\n",
    "#### 2. Accessing and Visualizing Satellite Imagery\n",
    "\n",
    "Geemap allows you to access and visualize satellite imagery from Google Earth Engine's vast [data catalog](https://developers.google.com/earth-engine/datasets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Landsat 8 collection\n",
    "landsat = (\n",
    "    ee.ImageCollection(\"LANDSAT/LC08/C02/T1_L2\")\n",
    "    .filterDate(\"2025-01-01\", \"2025-12-31\")\n",
    "    .filterBounds(ee.Geometry.Point(-122.085, 37.422))\n",
    "    .sort(\"CLOUD_COVER\")\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Define visualization parameters\n",
    "vis_params = {\"bands\": [\"SR_B4\", \"SR_B3\", \"SR_B2\"], \"min\": 7000, \"max\": 15000}\n",
    "\n",
    "# Add the image to the map\n",
    "m.add_layer(landsat, vis_params, \"Landsat 8\")\n",
    "m.center_object(landsat, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "This code performs a complex Earth Engine workflow:\n",
    "\n",
    "1. **Data Selection**:\n",
    "   - Accesses the Landsat 8 Collection 2 Level-2 dataset (`LANDSAT/LC08/C02/T1_L2`)\n",
    "   - Filters for images from 2025 (note: you should adjust this to use dates that have actual data)\n",
    "   - Filters for images that include the point near Google's headquarters in Mountain View, CA\n",
    "   - Sorts by cloud cover percentage (ascending, so least cloudy first)\n",
    "   - Takes the first (least cloudy) image from the filtered collection\n",
    "\n",
    "2. **Visualization Setup**:\n",
    "   - Defines how the image should be displayed:\n",
    "     - `'bands'`: Uses bands 4 (red), 3 (green), and 2 (blue) for a natural color display\n",
    "     - `'min'` and `'max'`: Sets the range for scaling pixel values to display colors\n",
    "     - These values are specific to the surface reflectance data in Landsat 8\n",
    "\n",
    "3. **Map Display**:\n",
    "   - Adds the image to the map with the specified visualization parameters\n",
    "   - Centers the map on the image at zoom level 8\n",
    "\n",
    "The key advantage here is that all data remains in Google's cloud. Your browser only receives the small tiles needed for display, not the full multi-gigabyte satellite image.\n",
    "\n",
    "To inspect the metadata of the Landsat image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "landsat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "This displays a representation of the Earth Engine Image object, showing properties like acquisition date, cloud cover percentage, satellite path/row, and other metadata.\n",
    "\n",
    "#### Computing and Visualizing Indices\n",
    "\n",
    "Geemap allows you to perform calculations on satellite imagery directly in the cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate NDVI (Normalized Difference Vegetation Index)\n",
    "ndvi = landsat.normalizedDifference([\"SR_B5\", \"SR_B4\"]).rename(\"NDVI\")\n",
    "\n",
    "# Define visualization parameters for NDVI\n",
    "ndvi_vis = {\"min\": -1, \"max\": 1, \"palette\": [\"blue\", \"white\", \"green\"]}\n",
    "\n",
    "# Add NDVI to the map\n",
    "m.add_layer(ndvi, ndvi_vis, \"NDVI\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "This code:\n",
    "\n",
    "1. **Calculates NDVI**:\n",
    "   - Uses the `normalizedDifference()` method to compute (NIR - Red) / (NIR + Red)\n",
    "   - NIR is Band 5 in Landsat 8\n",
    "   - Red is Band 4 in Landsat 8\n",
    "   - This calculation is performed for every pixel in the image on Google's servers\n",
    "   - The result is renamed to 'NDVI' for clarity\n",
    "\n",
    "2. **Defines Visualization**:\n",
    "   - NDVI ranges from -1 to 1, so those are set as the min/max values\n",
    "   - A color palette is defined where:\n",
    "     - Blue represents negative values (typically water)\n",
    "     - White represents values near zero (typically built-up areas, bare soil)\n",
    "     - Green represents positive values (typically healthy vegetation)\n",
    "\n",
    "3. **Adds to the Map**:\n",
    "   - Adds the NDVI layer to the map with the specified visualization parameters\n",
    "   - Names the layer 'NDVI' in the layer control panel\n",
    "\n",
    "This computation would require downloading gigabytes of satellite data if performed locally, but with Earth Engine, it runs in seconds on Google's infrastructure. The NDVI visualization makes vegetation health immediately apparent - brighter green areas have higher photosynthetic activity indicating healthier or denser vegetation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "##  STATE OF THE ART AND CURRENT DEVELOPMENTS\n",
    "\n",
    "Geospatial data visualization with Python has undergone a remarkable transformation, evolving from rudimentary mapping functionalities to a sophisticated ecosystem of interactive and cloud-native tools. In its early stages (2008-2013), libraries like Descartes and matplotlib's Basemap provided foundational mapping capabilities, albeit with limitations in interactivity and ease of use, often requiring significant expertise. A pivotal moment arrived in the middle period (2014-2019) with the emergence of GeoPandas, initially released in 2013, which seamlessly integrated spatial data handling with the powerful data manipulation capabilities of the pandas library. Simultaneously, Folium and ipyleaflet bridged the gap to Leaflet.js, enabling the creation of interactive web maps directly from Python environments. The current era (2020-present) is marked by a surge of user-friendly libraries such as leafmap, geemap, pydeck, kepler.gl, maplibre, and lonboard, alongside improved integration between these tools. Furthermore, the adoption of cloud-native geospatial formats like Cloud Optimized GeoTIFFs (COGs), GeoParquet, and SpatioTemporal Asset Catalogs (STAC) has revolutionized data storage, access, and dissemination. A key trend driving this evolution is the increasing prevalence of interactive and web-based visualizations. Static maps are increasingly being augmented or replaced by dynamic, browser-based tools like leafmap, folium, and pydeck, which generate interactive maps that can be readily embedded within web applications, Jupyter notebooks, and dashboards. These tools now support streaming data sources, facilitating real-time visualization for applications like traffic monitoring or weather pattern tracking. Emerging platforms like Hugging Face, Solara, and Py.Cafe are further expanding options for hosting geospatial data and visualizations. Complementing this is the rise of cloud-native geospatial approaches. Cloud Optimized GeoTIFFs (COGs) enable efficient access to portions of large raster datasets, allowing visualization without the need for complete downloads. SpatioTemporal Asset Catalogs (STAC) standardize the indexing and discovery of geospatial data, simplifying the process of finding and visualizing relevant data from extensive collections. Moreover, visualization platforms are increasingly integrating with cloud services for on-demand processing, utilizing serverless computing and formats like PMTiles to efficiently handle large datasets before visualization, showcasing a move towards scalable and accessible geospatial insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "## APPLICATION EXAMPLES\n",
    "\n",
    "In this section, we explore practical applications of geospatial analysis techniques using Python. We will demonstrate how to leverage the capabilities of various libraries to perform tasks such as land cover analysis, building height estimation, and 3D visualization of building data. Each example provides a step-by-step guide to implementing the techniques, along with explanations of the underlying concepts and code. These examples serve as a foundation for understanding how to apply geospatial analysis in real-world scenarios.\n",
    "\n",
    "### Required Packages\n",
    "\n",
    "Let's start by installing the necessary packages for this section. Uncomment the code below to install the required packages in your Python environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install maplibre rasterstats overturemaps rioxarray planetary-computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "This installation command adds specialized packages needed for our advanced examples:\n",
    "\n",
    "- `maplibre`: For 3D map visualization\n",
    "- `rasterstats`: For computing zonal statistics (calculations on raster data within vector boundaries)\n",
    "- `overturemaps`: A client for accessing Overture Maps building footprint data\n",
    "- `rioxarray`: For advanced raster analysis that combines rasterio with xarray\n",
    "- `planetary-computer`: For accessing Microsoft's Planetary Computer data catalog\n",
    "\n",
    "\n",
    "### Land Cover Analysis with Earth Engine\n",
    "\n",
    "In this section, we'll analyze land cover data from the National Land Cover Database (NLCD), which classifies land cover across the United States into different categories such as forest, urban, cropland, and water.\n",
    "\n",
    "#### Import Libraries\n",
    "\n",
    "First, we import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "#### Visualizing Land Cover Data\n",
    "\n",
    "Let's create an interactive map to visualize the NLCD land cover data across the United States:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an interactive map\n",
    "m = geemap.Map(center=[40, -100], zoom=4)\n",
    "\n",
    "# Add NLCD data\n",
    "dataset = ee.Image(\"USGS/NLCD_RELEASES/2019_REL/NLCD/2019\")\n",
    "landcover = dataset.select(\"landcover\")\n",
    "m.add_layer(landcover, {}, \"NLCD 2019\")\n",
    "\n",
    "# Add US census states\n",
    "states = ee.FeatureCollection(\"TIGER/2018/States\")\n",
    "style = {\"fillColor\": \"00000000\"}\n",
    "m.add_layer(states.style(**style), {}, \"US States\")\n",
    "\n",
    "# Add NLCD legend\n",
    "m.add_legend(title=\"NLCD Land Cover\", builtin_legend=\"NLCD\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "The resulting interactive map displays land cover patterns across the entire United States, with state boundaries overlaid for reference. Users can zoom and pan to explore different regions and use the legend to identify land cover types.\n",
    "\n",
    "#### Calculating Land Cover Statistics\n",
    "\n",
    "Next, we calculate the area of each land cover type across the United States:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = geemap.image_area_by_group(\n",
    "    landcover, scale=1000, denominator=1e6, decimal_places=4, verbose=True\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "| group | value | percentage |\n",
    "|----------------------|----------|-----------|\n",
    "| 1 | 1.416662e+05 | 0.0516 |\n",
    "| 2 | 4.320000e+02 | 0.0001 |\n",
    "| 2 | 1.610370e+04 | 0.0076 |\n",
    "| 2 | 7.856148e+04 | 0.0097 |\n",
    "| 2 | 6.729574e+04 | 0.0083 |\n",
    "| 2 | 1.808042e+04 | 0.0022 |\n",
    "| 3 | 6.326203e+04 | 0.0078 |\n",
    "| 4 | 8.412657e+05 | 0.1041 |\n",
    "| 4 | 1.002272e+06 | 0.1240 |\n",
    "| 4 | 1.991954e+05 | 0.0247 |\n",
    "| 5 | 1.784947e+06 | 0.2209 |\n",
    "| 7 | 1.058586e+06 | 0.1310 |\n",
    "| 8 | 5.370616e+05 | 0.0665 |\n",
    "| 8 | 1.501032e+06 | 0.1858 |\n",
    "| 9 | 3.649416e+05 | 0.0452 |\n",
    "| 9 | 8.557409e+04 | 0.0106 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "This function performs a comprehensive spatial analysis:\n",
    "\n",
    "1. It calculates the area occupied by each unique value (land cover class) in the landcover image\n",
    "2. The `scale` parameter (1000) determines the spatial resolution in meters at which to perform the calculation\n",
    "3. The `denominator` (1e6) converts square meters to square kilometers\n",
    "4. The result is a DataFrame showing each land cover class, its area in square kilometers, and its percentage of the total area\n",
    "\n",
    "The resulting DataFrame shows quantitative information about land cover distribution, such as how much of the US is covered by forests, cropland, developed areas, etc.\n",
    "\n",
    "Let's save these results to a CSV file for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/nlcd_stats.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "This saves the statistical results to a CSV file, allowing for further analysis in spreadsheet software or other data analysis tools.\n",
    "\n",
    "#### Adding Human-Readable Land Cover Class Names\n",
    "\n",
    "The NLCD classification uses numeric codes that aren't immediately intuitive. Let's create a legend mapping these codes to descriptive names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "legend = {\n",
    "    \"11\": \"Open Water\",\n",
    "    \"12\": \"Perennial Ice/Snow\",\n",
    "    \"21\": \"Developed, Open Space\",\n",
    "    \"22\": \"Developed, Low Intensity\",\n",
    "    \"23\": \"Developed, Medium Intensity\",\n",
    "    \"24\": \"Developed, High Intensity\",\n",
    "    \"31\": \"Barren Land (Rock/Sand/Clay)\",\n",
    "    \"41\": \"Deciduous Forest\",\n",
    "    \"42\": \"Evergreen Forest\",\n",
    "    \"43\": \"Mixed Forest\",\n",
    "    \"51\": \"Dwarf Scrub\",\n",
    "    \"52\": \"Shrub/Scrub\",\n",
    "    \"71\": \"Grassland/Herbaceous\",\n",
    "    \"72\": \"Sedge/Herbaceous\",\n",
    "    \"73\": \"Lichens\",\n",
    "    \"74\": \"Moss\",\n",
    "    \"81\": \"Pasture/Hay\",\n",
    "    \"82\": \"Cultivated Crops\",\n",
    "    \"90\": \"Woody Wetlands\",\n",
    "    \"95\": \"Emergent Herbaceous Wetlands\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "This dictionary maps each NLCD class code to its official name. For example, code '11' represents 'Open Water' (lakes, rivers, oceans), while code '42' represents 'Evergreen Forest'.\n",
    "\n",
    "Now we add these descriptive names to our DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"] = df.index.map(legend)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "This creates a new column called 'class' in our DataFrame, containing the descriptive name corresponding to each numeric class code. This makes the data more interpretable and ready for visualization.\n",
    "\n",
    "#### Visualizing Land Cover Distribution\n",
    "\n",
    "Data visualization helps us understand patterns and proportions more intuitively. Let's create a pie chart:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "geemap.pie_chart(df, names=\"class\", values=\"area\", height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75",
   "metadata": {},
   "source": [
    "This function creates an interactive pie chart where:\n",
    "\n",
    "- Each slice represents a land cover class\n",
    "- The size of each slice is proportional to its area\n",
    "- The labels show the descriptive names from our legend\n",
    "- The height parameter sets the chart's height in pixels\n",
    "\n",
    "This visualization immediately reveals which land cover types dominate the US landscape. For example, it might show that shrub/scrub, croplands, and grasslands occupy the largest portions of the country.\n",
    "\n",
    "#### Analyzing Land Cover by State\n",
    "\n",
    "While national statistics provide an overview, analyzing land cover by state reveals regional patterns and differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = geemap.zonal_stats_by_group(\n",
    "    landcover,\n",
    "    states,\n",
    "    stat_type=\"SUM\",\n",
    "    denominator=1e6,\n",
    "    decimal_places=2,\n",
    "    return_fc=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "This powerful function performs a complex geospatial analysis:\n",
    "\n",
    "1. It overlays the NLCD landcover data with state boundaries\n",
    "2. For each state, it calculates the area of each land cover class\n",
    "3. The results are calculated in Earth Engine's cloud infrastructure\n",
    "4. Setting `return_fc=True` returns an Earth Engine FeatureCollection\n",
    "\n",
    "This analysis would be computationally intensive to perform locally, especially with high-resolution data like NLCD, but Earth Engine makes it efficient by distributing the computation across Google's infrastructure.\n",
    "\n",
    "Let's convert the Earth Engine feature collection to a pandas DataFrame for easier analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = geemap.ee_to_df(stats)\n",
    "df_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "This converts the Earth Engine FeatureCollection to a pandas DataFrame, bringing the data into a tabular format that's easier to work with for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "geemap.bar_chart(\n",
    "    df_stats,\n",
    "    x=\"STUSPS\",\n",
    "    y=\"Class_82\",\n",
    "    title=\"Cropland Area by State\",\n",
    "    x_label=\"State\",\n",
    "    y_label=\"Area (km2)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "This creates a bar chart showing:\n",
    "\n",
    "- States (using their USPS abbreviations) on the x-axis\n",
    "- Cropland area (Class 82 in NLCD) on the y-axis\n",
    "- Appropriate title and axis labels\n",
    "\n",
    "This visualization reveals which states have the most cropland, providing insights into the geographic distribution of agricultural activity across the United States. To visualize other land cover classes, simply change the `y` parameter in the `bar_chart()` function to the desired class code.\n",
    "\n",
    "We can also visualize the same data as a pie chart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "geemap.pie_chart(\n",
    "    df_stats,\n",
    "    names=\"STUSPS\",\n",
    "    values=\"Class_82\",\n",
    "    title=\"Cropland Area by State\",\n",
    "    legend_title=\"State\",\n",
    "    height=600,\n",
    "    max_rows=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "This pie chart shows:\n",
    "\n",
    "- The proportion of US cropland in each state\n",
    "- State abbreviations as labels\n",
    "- A legend with state names\n",
    "- A height of 600 pixels to accommodate the detailed breakdown\n",
    "- Maximum of 30 rows in the legend to ensure all states are visible\n",
    "\n",
    "We can clearly see that Iowa, Kansas, and North Dakota have the largest cropland areas, while states like New York and Pensylvania have significantly less. The pie chart provides a clear visual representation of the relative proportions of cropland in each state, making it easy to identify which states dominate in agricultural land use.\n",
    "\n",
    "Together, these visualizations provide complementary views of agricultural land distribution: the bar chart emphasizes the absolute area of cropland in each state, while the pie chart highlights the relative proportion of cropland among states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "### Estimating Building Heights with Planetary Computer\n",
    "\n",
    "In this section, we demonstrate how to estimate building heights using LiDAR data from Microsoft's Planetary Computer and building footprints from Overture Maps. This integration of diverse geospatial datasets enables detailed 3D urban analysis.\n",
    "\n",
    "#### Import Libraries\n",
    "\n",
    "First, we import the necessary modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leafmap\n",
    "from leafmap.download import (\n",
    "    pc_stac_search,\n",
    "    pc_stac_download,\n",
    "    read_pc_item_asset,\n",
    "    sign_pc_item,\n",
    "    extract_building_stats,\n",
    "    get_overture_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "These imports include specialized functions from leafmap's download module:\n",
    "\n",
    "- `pc_stac_search`: Searches for data in the Planetary Computer catalog using the STAC API\n",
    "- `pc_stac_download`: Downloads data from Planetary Computer\n",
    "- `read_pc_item_asset`: Loads a specific asset from a Planetary Computer item\n",
    "- `sign_pc_item`: Generates authenticated URLs for Planetary Computer data access\n",
    "- `get_overture_data`: Retrieves building footprint data from Overture Maps\n",
    "- `extract_building_stats`: Extracts statistics from building footprint data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "#### Defining the Study Area\n",
    "\n",
    "We'll define a bounding box for our study area near Washington DC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = leafmap.Map(center=[38.8343, -77.1632], zoom=13)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "This creates an interactive map centered on Arlington, Virginia, near Washington DC. This location contains a mix of building types and heights, making it a good example for height analysis.\n",
    "\n",
    "We can either draw a region of interest on the map or use a predefined bounding box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = m.user_roi_bounds()\n",
    "if bbox is None:\n",
    "    bbox = [-77.2066, 38.8274, -77.1847, 38.8364]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "This code:\n",
    "\n",
    "1. Attempts to get a user-drawn region of interest using `user_roi_bounds()`\n",
    "2. If no region was drawn, falls back to a predefined bounding box\n",
    "3. The bounding box coordinates represent [west, south, east, north] boundaries in decimal degrees\n",
    "\n",
    "This approach gives users flexibility in selecting their study area while ensuring the code runs smoothly even without user input.\n",
    "\n",
    "#### Downloading LiDAR Height Above Ground (HAG) Data\n",
    "\n",
    "Now we search for and download LiDAR-derived Height Above Ground (HAG) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = pc_stac_search(\n",
    "    collection=\"3dep-lidar-hag\", bbox=bbox, time_range=\"2018-01-01/2018-12-31\", limit=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {},
   "source": [
    "This function:\n",
    "\n",
    "1. Queries the Planetary Computer catalog for data from the \"3dep-lidar-hag\" collection\n",
    "2. Filters for data that intersects our bounding box\n",
    "3. Limits the search to data collected in 2018\n",
    "4. Returns at most one item to keep the example focused\n",
    "\n",
    "The 3DEP (3D Elevation Program) LiDAR HAG dataset is derived from high-resolution LiDAR point clouds collected by the USGS. The HAG data specifically represents the height of objects above the ground surface, making it ideal for extracting building heights.\n",
    "\n",
    "Let's examine the metadata of the first item found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "items[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "This displays detailed metadata about the dataset, including:\n",
    "\n",
    "- A unique identifier\n",
    "- Temporal and spatial information about when and where the data was collected\n",
    "- Available data assets (e.g., the HAG raster, thumbnails, metadata files)\n",
    "- Data quality indicators and processing information\n",
    "\n",
    "Understanding this metadata helps us assess the dataset's suitability for our analysis.\n",
    "\n",
    "Now we can visualize this data on the map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = leafmap.Map()\n",
    "m.add_basemap(\"Esri.WorldImagery\")\n",
    "m.add_stac_layer(\n",
    "    collection=\"3dep-lidar-hag\",\n",
    "    item=items[0].id,\n",
    "    assets=[\"data\"],\n",
    "    name=\"Height Above Ground (HAG)\",\n",
    ")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "The map displays the HAG data as a raster overlay on satellite imagery. Brighter areas typically represent taller objects (buildings, trees), while darker areas represent ground level. This visual inspection helps us assess the data quality and coverage. Note that the data is rendered on the fly, so you can pan and zoom to explore different areas interactively. This is particularly useful when visualizing large datasets, as it allows you to focus on specific regions of interest without downloading the entire dataset.\n",
    "\n",
    "Alternatively, you can download the data to your local machine for offline analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_stac_download(\n",
    "    items[0],\n",
    "    output_dir=\"data\",\n",
    "    assets=[\"data\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "We can generate a signed URL for accessing the data programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "href = sign_pc_item(\n",
    "    items[0],\n",
    "    asset=\"data\",\n",
    ")\n",
    "href"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "This creates a temporary authenticated URL that provides access to the data even from scripts running outside the notebook. The signing process ensures that even though the data is stored in cloud storage, it can be accessed securely by authorized users.\n",
    "\n",
    "Alternatively, we can load the HAG data directly into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "hag = read_pc_item_asset(items[0], asset=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "This reads the HAG data into an xarray DataArray, which can be used for further analysis in Python. Loading the data into memory is useful for small areas, but for larger regions, it's more efficient to work with the data through URLs or local files.\n",
    "\n",
    "#### Downloading Building Footprint Data\n",
    "\n",
    "Next, we download building footprint data from Overture Maps for the same area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = get_overture_data(\n",
    "    overture_type=\"building\", bbox=bbox, columns=[\"height\", \"geometry\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "This function:\n",
    "\n",
    "1. Connects to the Overture Maps database\n",
    "2. Requests building footprints within our defined bounding box\n",
    "3. Retrieves only the specified columns (\"height\" and \"geometry\") to minimize data transfer\n",
    "4. Returns the data as a GeoDataFrame\n",
    "\n",
    "Overture Maps is an open data collaboration that provides global building footprints with various attributes. The \"height\" attribute is populated for some buildings based on existing data sources, but many buildings lack height information - which is why we'll use the LiDAR data to fill these gaps.\n",
    "\n",
    "Let's examine the building data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "This displays the first few rows of the GeoDataFrame, showing:\n",
    "\n",
    "- The height attribute (when available, often null)\n",
    "- The geometry column containing polygon shapes that represent building footprints\n",
    "\n",
    "We save the building data for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(\"data/buildings.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "This saves the building footprints as a GeoJSON file, which can be shared or used in other GIS applications. GeoJSON is a standard format for geospatial vector data that's widely supported by mapping libraries and GIS software.\n",
    "\n",
    "#### Analyzing Building Statistics\n",
    "\n",
    "Now we extract basic statistics about the buildings in our study area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = extract_building_stats(gdf)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "This function analyzes the building footprint data and returns key statistics like the total number of buildings, and the number of buildings with height information.\n",
    "\n",
    "Let's add the building footprints to our map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_gdf(gdf, layer_name=\"Buildings\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "This adds the building footprint polygons to our map as an overlay. Use the layer control panel to toggle the visibility of the building layer, HAG layer, and basemapS. This allows you to visualize the building footprints in relation to the HAG data and the underlying satellite imagery.\n",
    "\n",
    "#### Calculate Building Heights from LiDAR Data\n",
    "\n",
    "Now we use zonal statistics to calculate height statistics for each building from the LiDAR data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = leafmap.zonal_stats(\n",
    "    gdf, href, stats=[\"min\", \"max\", \"mean\", \"median\", \"count\"], gdf_out=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "This function:\n",
    "\n",
    "1. Uses the building footprints (polygons) from our GeoDataFrame\n",
    "2. For each building, extracts all HAG pixel values that fall within its footprint\n",
    "3. Calculates statistical measures for those values:\n",
    "   - Min: Lowest height value (often near zero due to edge effects or ground points)\n",
    "   - Max: Highest height value (typically representing the roof peak or highest structure)\n",
    "   - Mean: Average height across the building footprint\n",
    "   - Median: Middle value, often more representative for buildings with complex roofs\n",
    "   - Count: Number of HAG pixels within the building footprint\n",
    "4. Returns the original GeoDataFrame with these statistics added as new columns\n",
    "\n",
    "This operation effectively extracts 3D information (height) from the 2D building footprints by incorporating the LiDAR-derived height data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "This displays the GeoDataFrame with the newly added height statistics columns. We can now see the height characteristics of each building in our dataset.\n",
    "\n",
    "Let's visualize the distribution of building heights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[\"median\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119",
   "metadata": {},
   "source": [
    "This creates a histogram showing the distribution of median building heights in our study area. The distribution might reveal patterns like:\n",
    "\n",
    "- Most buildings clustering around certain heights (e.g., single-family homes around 5-10 meters)\n",
    "- Taller buildings forming a separate cluster or long tail\n",
    "- Bimodal or multimodal distributions suggesting different building types\n",
    "\n",
    "Now we update our GeoDataFrame with the median height values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf[\"height\"] = stats[\"median\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "This adds or updates the 'height' column in our original GeoDataFrame with the median height values calculated from the LiDAR data. We use the median rather than the mean because it's less influenced by outliers or noise in the LiDAR data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "building_height = \"data/building_height.geojson\"\n",
    "gdf[[\"height\", \"geometry\"]].to_file(building_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "This code saves the data as a GeoJSON file. The resulting file contains building footprints with their estimated heights, ready for 3D visualization or further analysis. This streamlined dataset is easier to share and work with than the full dataset with all statistical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "### Visualizing Building Height Data in 3D\n",
    "\n",
    "In this final section, we create interactive 3D visualizations of building data. 3D visualization helps urban planners, architects, and analysts understand the vertical dimension of the built environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import leafmap.maplibregl as leafmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126",
   "metadata": {},
   "source": [
    "This imports the maplibregl submodule of leafmap, which provides 3D mapping capabilities using the MapLibre GL JS library. Unlike the standard leafmap module which is based on ipyleaflet, the maplibregl submodule enables advanced 3D visualizations including building extrusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "#### 2D Visualization of Building Heights\n",
    "\n",
    "\n",
    "First, let's create a 2D map with buildings colored by height:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = leafmap.Map()\n",
    "m.add_data(building_height, column=\"height\")\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "This creates a 2D thematic map where:\n",
    "\n",
    "1. Building footprints are displayed as filled polygons\n",
    "2. The color of each building corresponds to its height\n",
    "3. A color scale (legend) shows the mapping between colors and height values\n",
    "\n",
    "This 2D visualization provides a quick overview of height patterns across the study area. Taller buildings typically appear in warmer colors (reds, oranges), while shorter buildings appear in cooler colors (blues, greens).\n",
    "\n",
    "#### 3D Extrusion of Buildings\n",
    "\n",
    "Now we create a 3D visualization by extruding buildings based on their height:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = leafmap.Map(pitch=60, bearing=30)\n",
    "m.add_data(building_height, column=\"height\", cmap=\"Blues\", extrude=True)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "This creates a 3D visualization where:\n",
    "1. Buildings are extruded (given height) based on their median HAG value\n",
    "2. The map view is tilted (pitch=60 degrees) and rotated (bearing=30 degrees) to provide a 3D perspective\n",
    "3. Buildings are colored using the \"Blues\" colormap, with darker blues for taller buildings\n",
    "4. The `extrude=True` parameter is the key setting that creates the 3D effect\n",
    "\n",
    "This 3D visualization makes it much easier to identify tall buildings and understand the vertical structure of the urban environment. Users can navigate this map interactively, changing the viewpoint to explore different perspectives on the urban morphology.\n",
    "\n",
    "For comparison, let's also look at a different area with pre-existing building height data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = leafmap.Map(\n",
    "    center=[-74.0095, 40.7046], zoom=16, pitch=60, bearing=-17, style=\"positron\"\n",
    ")\n",
    "m.add_basemap(\"Esri.WorldImagery\", visible=False)\n",
    "m.add_overture_3d_buildings(template=\"simple\")\n",
    "m.add_layer_control()\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133",
   "metadata": {},
   "source": [
    "This code creates a 3D visualization of buildings in Lower Manhattan, New York City:\n",
    "\n",
    "1. Centers the map on the Financial District area\n",
    "2. Sets an appropriate zoom level, pitch, and bearing for 3D viewing\n",
    "3. Uses the \"positron\" basemap style for a clean background\n",
    "4. Adds satellite imagery as an optional basemap (default hidden)\n",
    "5. Uses the `add_overture_3d_buildings` function to add 3D buildings from Overture Maps\n",
    "6. Adds a layer control to toggle between different layers\n",
    "\n",
    "The `template=\"simple\"` parameter uses a simplified visualization style suitable for displaying the urban form without overwhelming visual details. The resulting 3D view provides an intuitive representation of the iconic Manhattan skyline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "## BEST PRACTICES AND OPEN ISSUES\n",
    "\n",
    "### Vector Data Visualization Best Practices\n",
    "\n",
    "Selecting the appropriate format for vector data is critical for efficient visualization and analysis in geospatial applications. The [Cloud-Native Geospatial Formats Guide](https://guide.cloudnativegeo.org) provides comprehensive recommendations for optimal data handling across various use cases. For small vector datasets, GeoJSON remains the preferred format due to its simplicity, wide support, and human-readable structure. However, as datasets grow in size and complexity, alternatives become necessary. Large vector datasets benefit from formats like GeoJSONL (for streaming processing), GeoPackage (for local storage with spatial indexing), GeoParquet (for distributed analysis), or FlatGeoBuf (for efficient random access). When optimizing for cloud storage and analytical queries, GeoParquet stands out by combining columnar storage with spatial indexing capabilities, enabling efficient filtering and aggregation operations across distributed computing environments.\n",
    "\n",
    "Visualization of vector data introduces additional considerations, particularly when working with web maps that must maintain responsiveness across different zoom levels and extents. PMTiles has emerged as a particularly effective solution for visualizing large vector datasets, offering progressive loading and efficient caching mechanisms. This format enables smooth visualization experiences even when working with millions of features. Beyond format selection, geometry simplification represents another crucial optimization technique. By reducing vertex counts while preserving essential shape characteristics, simplified geometries dramatically reduce file sizes and improve rendering performance. This process should be applied dynamically based on the visualization scale – using more detailed geometries at closer zoom levels and simplified versions for overview maps. For processing extensive vector datasets, combining GeoPandas with distributed computing frameworks like Dask or Apache Sedona enables scalable operations that would otherwise be impossible on a single machine.\n",
    "\n",
    "### Raster Data Visualization Best Practices\n",
    "\n",
    "Cloud Optimized GeoTIFFs (COGs) have revolutionized how we work with large raster datasets by enabling efficient, partial access to remote data through HTTP range requests. This format organizes data into internal tiles with overview levels, allowing applications to retrieve only the specific portions needed for visualization at the current zoom level. Tools like `leafmap.cog_validate()` help ensure raster datasets conform to the COG specification, while `leafmap.image_to_cog()` provides a straightforward way to convert conventional GeoTIFFs to this optimized format. When working with extensive collections of raster data, SpatioTemporal Asset Catalogs (STAC) provide a standardized approach for organizing, discovering, and accessing geospatial assets. This metadata specification facilitates efficient searching across multiple dimensions including spatial extent, temporal range, and data characteristics.\n",
    "\n",
    "For analysis and manipulation of raster data, rasterio offers a Pythonic interface to the GDAL library, while rioxarray extends these capabilities by integrating with the powerful labeled multidimensional array operations of xarray. This combination is particularly valuable when working with time series of raster data or multi-dimensional scientific datasets. When processing exceeds the capacity of a single machine, frameworks like Dask enable parallel operations through chunked arrays, while the zarr format provides efficient storage and access for distributed computing. These technologies make it possible to visualize and analyze terabyte-scale datasets that would otherwise be unmanageable. Effective visualization of raster data also requires thoughtful color mapping and contrast enhancement to highlight relevant patterns and features while maintaining perceptual accuracy – considerations that become increasingly important when visualizing multispectral or hyperspectral imagery.\n",
    "\n",
    "### Challenges and Limitations\n",
    "\n",
    "Despite significant advances in geospatial visualization technologies, several persistent challenges limit their effectiveness in critical applications. Access to high-resolution satellite imagery remains constrained during disaster response scenarios when such data is most urgently needed. Complex licensing agreements, costs, and technical barriers often prevent timely distribution of imagery to emergency responders and affected communities when every minute counts. While Google Earth Engine provides extraordinary capabilities for large-scale geospatial analysis, its proprietary nature raises concerns about long-term accessibility and vendor lock-in. The recent discontinuation of Microsoft's Planetary Computer JupyterHub highlights the vulnerability of relying on corporate platforms for essential research infrastructure. Open-source alternatives exist but typically require substantial technical expertise to deploy and maintain.\n",
    "\n",
    "Computational resources present another significant limitation, particularly for advanced visualization and analysis techniques. GPU acceleration has become essential for deep learning applications and complex 3D visualizations, yet access to these resources remains limited in free computing environments like Google Colab. This creates an accessibility gap that disproportionately affects researchers and practitioners with limited institutional resources. The geospatial AI ecosystem, while growing rapidly, still lacks user-friendly packages that make advanced techniques accessible to domain experts without extensive programming experience. This gap between cutting-edge research and practical applications slows the adoption of innovative approaches in real-world scenarios.\n",
    "\n",
    "Support for specialized data types represents a final frontier in geospatial visualization. Three-dimensional point clouds from LiDAR and photogrammetry contain rich information but remain challenging to visualize and analyze efficiently, especially at large scales. Time series visualization presents similar challenges when attempting to represent both spatial and temporal patterns simultaneously. Interactive animations and linked views offer promising approaches but require careful design to remain interpretable. As these data types become increasingly common in environmental monitoring, urban planning, and climate science, developing more effective visualization techniques will be essential for extracting meaningful insights from complex spatiotemporal datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "## FUTURE IMPLICATIONS\n",
    "\n",
    "The future of geospatial technology is inextricably linked to advancements in artificial intelligence, promising a paradigm shift in how we analyze, visualize, and interact with spatial data. Large Language Models (LLMs) are poised to revolutionize geospatial workflows, enabling natural language queries for complex analysis, automated report generation, and intelligent data exploration. Complementing this, the development of geospatial foundation models—AI models specifically trained on vast geospatial datasets—will provide a powerful base for diverse applications, from automated feature extraction to predictive modeling. Enhancing accessibility is also paramount, with the rise of low-code and no-code platforms empowering users of all skill levels to harness the power of geospatial analysis. A thriving Jupyter ecosystem, with specialized extensions and seamless geospatial data integration, will further streamline development and collaboration. Lastly, the emergence of community-driven geospatial data catalogs will be critical for ensuring open access to the wealth of geospatial information, fostering innovation and collaborative problem-solving. These converging trends herald a future where geospatial technology is more intelligent, accessible, and collaborative, driving impactful solutions across diverse domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "## REFERENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
